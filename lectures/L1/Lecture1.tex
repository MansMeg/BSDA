%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\input{../common/lststan} % Stan listing

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage{../common/beamerthemeUppsala}
%\usetheme{Uppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

% Input new commands
\input{../common/commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Bayesian Statistics and Data Analysis \\ Lecture 1}}
\author[]{M{\aa}ns Magnusson \\ Department of Statistics, Uppsala University \\ Thanks to Aki Vehtari, Aalto University}
\date{}

\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\frame{\sectionpage}

\begin{frame}

  \frametitle{Decision making in case of uncertainties}

  \begin{center}
    \includegraphics[width=10.5cm]{figs/irma.png}
  \end{center}
\end{frame}


\begin{frame}{Bayesian Analysis}

  \begin{itemize}
  \item Bayesian probability theory
    \begin{itemize}
    \item uncertainty is presented with probabilities
    \item probabilities are updated based on new information
    % \item {\it ...common sense reduced to calculation},
    %   Laplace 1819
    \end{itemize}
    \pause
    \item Thomas Bayes (170?--1761)
    \begin{itemize}
    \item English nonconformist, Presbyterian minister,
      mathematician
    % \item Richard Price published Bayes' paper on conditional
    %   probabilities in 1763 after Bayes had died
    \item considered the problem of {\it inverse probability}
      \begin{itemize}
        \item significant part of the Bayesian theory
      \end{itemize}
  \end{itemize}
  \pause
  \item Bayes did not invent all, but was first to solve problem of
    inverse probability in special case
  \item Modern Bayesian theory with rigorous proofs developed in
    20th century
\end{itemize}
\end{frame}

\begin{frame}{Term Bayesian used first time in mid 20th century}

  \begin{itemize}
  \item Earlier there was just "probability theory"
    \begin{itemize}
    \item concept of the probability was not strictly defined,
      although it was close to modern Bayesian interpretation
    \item in the end of 19th century there were increasing demand
      for more strict definition of probability (mathematical and
      philosophical problem)
    \end{itemize}
    \pause
    \item In the beginning of 20th century frequentist view gained popularity
    \begin{itemize}
    \item accepts definition of probabilities only through frequencies
    \item does not accept inverse probability or use of prior
    \item gained popularity due to apparent objectivity and "cook
      book" like reference books
    \end{itemize}
    \pause
    \item R. A. Fisher used in 1950 first
      time term "Bayesian" to emphasize the difference to general
      term "probability theory"
      \begin{itemize}
      \item term became quickly popular, because alternative
        descriptions were longer
    %     \pause
    % \item after this Bayesians started to use term "frequentist"
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Bayesian core: Uncertainty and probabilistic modeling}

  \begin{itemize}
%  \item Two types of uncertainty: aleatoric and epistemic
%    \vspace{\baselineskip}
  \item Representing uncertainty with probabilities
    \vspace{\baselineskip}
  \item Updating uncertainty (with empirical observations)
   \end{itemize}
\end{frame}


\begin{frame}{Two types of uncertainty}

  \begin{itemize}
  \item Aleatoric uncertainty due to randomness
    \begin{itemize}
    \item<1-> we are not able to obtain observations which could reduce
      this uncertainty
    \end{itemize}
    \vspace{\baselineskip}
  \item Epistemic uncertainty due to lack of knowledge
    \begin{itemize}
    \item<2-> we are able to obtain observations which can reduce
      this uncertainty
    \item<3-> two observers may have different epistemic uncertainty
    \end{itemize}
  \end{itemize}
\end{frame}


\section{Bayesian Reasoning}
\frame{\sectionpage}

\begin{frame}{Probability in Bayesian Reasoning}

  \begin{itemize}
  \item Uncertainty and knowledge is expressed as probability distributions, why? \pause
    \item Analogy to physical randomness
    \item Knowledge as coherence of bets (the Dutch book)
     \\ Example: If you say: P(rain) = 0.7 and P(no rain) = 0.4, those add up to 1.1 (too high).\\
A bookmaker could then take both bets from you and be sure to profit, because you’ve made your odds \\ \uured{incoherent}.
  \pause
  \item Everyone can have their own 'subjective' uncertainty, e.g.  P(rain tomorrow), P(Magdalena Andersson will be the next primeminister)
  \item Frequency arguments can be difficult in some situations: P(other life in the universe)
  \end{itemize}
\end{frame}

\begin{frame}{Bayesian epistemiology}

\pause

% the theory of knowledge, especially with regard to its methods, validity, and scope, and the distinction between justified belief and opinion


  \begin{itemize}
    \item What is \uured{epistemiology}?
    \pause
    \\ "the theory of knowledge" (Wikipedia)
    \pause
    \item Bayesian epistemiology:
    \pause
    \begin{itemize}
    \item State of knowledge is a probability distribution,
    \item e.g. unlike Popperian aproaches, we can talk about P(black swan)
    \item (New) Research in Philisophy of Science
    \end{itemize}
  \end{itemize}
\end{frame}


\section{Bayesian Statistical inference}
\frame{\sectionpage}

\begin{frame}{Statistical inference}
  \begin{itemize}
  \item Draw conclusions of {\color{uured} unobserved} entities, based on {\color{uured} data} \pause
  \item Different types of unobserved entities
    \begin{itemize}
    \item {\color{uured} potentially observed}: future observations, treatment effects
    \item {\color{uured} parameters}: data-generating process (e.g. regression coefficients)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Notation}

  \begin{itemize}
    \item $p(\cdot|\cdot)$: conditional pdf/pmf
    \item $p(\cdot)$: marginal pdf/pmf
    \item $P(\cdot)$ or $Pr(\cdot)$: probability, e.g. $P(\theta > 0) = \int_0^\infty p(\theta) d\theta$
    \item Random variable: $\theta \sim N(\mu,\sigma)$
    \item pdf/pmf: $p(\theta) = N(\theta | \mu,\sigma)$
  \end{itemize}

\end{frame}

\begin{frame}{Parameters, data, and predictions}

  \begin{itemize}
  \item Model parameters: $\theta = (\theta_1,...,\theta_p)$
  \item Observed data: $y = (y_1,...,y_n)$
    \begin{itemize}
    \item $y_i$ can be a vector and is assumed to come from a \uured{data generating process} (probabilistic model)
    \end{itemize}
  \item Potentially observed data: $\tilde{y} = (\tilde{y}_1,...,\tilde{y}_m)$
  \item Observed (known) covariates: $x$
  \pause
  \item I'm sloppy: e.g. often implicitly condition on $x$, i.e. $p(\theta|y) = p(\theta|y, x)$
%  \pause
%  \item We assume \emph{exchangeability} of the observations (more later):
%\[
%p(y_1,...,y_n) = p(y_\pi(1),...,y_\pi(n))
%\]
  \end{itemize}
\end{frame}



\begin{frame}{The basic steps of Bayesian inference}

  \begin{enumerate}
  \item Setting up a full probability model $p(y|\theta) \cdot p(\theta) = p(y,\theta)$ %; a joint probability distribution for all observable and unobservable quantities in a problem. The model should be consistent with knowledge about the underlying scientific problem and the data collection process.
  \pause
  \item Conditioning on observed data $y$ to calculate the posterior distribution $p(\theta|y)$%: calculating and interpreting the appropriate posterior distribution—the conditional probability distribution of the unobserved quantities of ultimate interest, given the observed data.
  \pause
  \item Evaluate the model. If not satisfied, go back to 1. Else use the model.% Evaluating the fit of the model and the implications of the resulting posterior distribution: how well does the model fit the data, are the substantive conclusions reasonable, and how sensitive are the results to the modeling assumptions in step 1? In response, one can alter or expand the model and repeat the three steps.
   \end{enumerate}
\end{frame}


\begin{frame}{Bayesian Inference I}
  \begin{itemize}
  \item Bayesian conclusions about $\theta$ or $\tilde{y}$ are made using probabilities, {\color{uured} conditional on data} $y$
  \pause
  \item We state our uncertainty about $\theta$ or $\tilde{y}$ as \uured{distributions}
  \pause
  \begin{itemize}
    \item potentially observed: $p(\tilde{y}|y)$
    \item parameters: $p(\theta|y)$
  \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}{Bayesian Inference: Setting up the model}
  \begin{itemize}
  \item Start out with a {\color{uured} model}: a \emph{joint distribution} for data and parameters:
  \[
  p(y,\theta) = p(y|\theta) p(\theta)
  \]
  \pause
  \item $p(y|\theta)$ is our data model, and when conditioned on $y$, the \emph{likelihood}
  \pause
  \item $p(\theta)$ is our prior distribution for our parameters
  \end{itemize}
\end{frame}

\begin{frame}{Bayesian Inference: Computing the posterior}
  \begin{itemize}
  \item Conditioning on data $y$, using \emph{Bayes theorem}, we can the compute the \emph{posterior distribution}
  \[
  p(\theta|y) = \frac{p(y|\theta) p(\theta)}{p(y)}
  \]
  where
  \[
  p(y) = \int p(y|\theta) p(\theta) d\theta
  \]
  \pause
  \item $p(y)$ is the \emph{marginal likelihood}
  \pause
  \item $p(\theta|y)$ summarize our knowledge about $\theta$
  \pause
  \item Bayesian statistics obey the \emph{likelihood principle}: \\ data only affects $p(\theta|y)$ through the likelihood $p(y|\theta)$
  \end{itemize}
\end{frame}


\begin{frame}{Predictions}
  \begin{itemize}
  \item How to do inference on an unknown (potential) observable $\tilde{y}$? E.g. a future observation
  \pause
  \item We use our \emph{data} model and our posterior and \emph{marginalize} over the uncertainty
  \[
  p(\tilde{y}|y) = \int p(\tilde{y}|\theta) p(\theta|y) d\theta
  \]
  \item The \emph{posterior predictive distribution}
  % \item 'An average of conditional predictions over the posterior distribution'
  \end{itemize}
\end{frame}


\begin{frame}{Example: Updating uncertainty}

  \begin{itemize}
  %\item<2-> probability of red $\frac{\mathrm{\#red}}{\mathrm{\#red+\#yellow}}{\onslide<3->=\theta}$
  \item<2-> Probability of red $\frac{\mathrm{\#red}}{\mathrm{\#red+\#yellow}}=\theta$
    \vspace{\baselineskip}
  \item<3-> $p(y=\mathrm{red}|\theta)=\theta \quad$ aleatoric uncertainty
    \vspace{\baselineskip}
  \item<4-> $p(\theta) \quad$ epistemic uncertainty
    \vspace{\baselineskip}
  \item<5-> Picking many chips updates our uncertainty about the proportion
    \vspace{\baselineskip}
  \item<5-> $p(\theta|\mathrm{y=red,yellow,red,red,\ldots})=?$
    \vspace{\baselineskip}
  \item<6-> Bayes rule
      $p(\theta|y)=\frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta}$
  \end{itemize}
\end{frame}


\begin{frame}{Model vs. Likelihood}
  \begin{itemize}
  \item Bayes rule
      \[
      p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} \propto p(y|\theta)p(\theta)
      \]
    \vspace{\baselineskip}
  \item Data model: $p(y|\theta)$ as a function of $y$ given fixed $\theta$
    describes the {\color{uured} aleatoric} uncertainty \vspace{\baselineskip}
  \item Likelihood: $p(y|\theta)=L(\theta|y)$
    as a function of $\theta$
    given fixed $y$ provides information about epistemic uncertainty.%,
    but is not a probability distribution. %, why?
    \vspace{\baselineskip}
  \item<2-> Bayes rule combines the likelihood with prior uncertainty
    $p(\theta)$ and transforms them to updated posterior uncertainty $p(\theta|y)$
  \end{itemize}
\end{frame}



\begin{frame}{Example application: Effects on roaches}
% https://avehtari.github.io/modelselection/roaches.html
\begin{itemize}
\item Question: Effect of treamtments on captured roaches
\pause
\item Outcome ($y$): The number of roaches
\item Coeffcients ($x$): Treatment, Senior home, Pre-treatment number of roaches
\pause
\item Data model:
\[
p(y|\theta) = Po(\lambda) = \frac{1}{y!}\lambda^y \exp(\lambda)
\]
where
\[
\lambda = \exp(\alpha + \beta X)
\]
\item Prior:
\[
p(\theta) = p(\alpha) = p(\beta_k) = N(\theta|\mu,\sigma)
\]
\pause
\item Posterior:
\[
p(\alpha, \beta|y) \propto N(\alpha|\mu,\sigma) \prod^K N(\beta_k|\mu,\sigma) \prod^N_{i=1} Po(\exp(\alpha + \beta x))
\]
\pause
\item Predictive distribution:
\[
p(\tilde{y}|y,\tilde{x}) = \int Po(\exp(\alpha + \beta \tilde{x})) p(\alpha, \beta |y) d\alpha d\beta
\]

\end{itemize}

\end{frame}

\section{Probabilistic Modeling}
\frame{\sectionpage}

\begin{frame}{The art of probabilistic modeling}
  \begin{itemize}
  \item Subjectivity: we need to specify both $p(\theta)$ and $p(y|\theta)$
  \item The art of probabilistic modeling is to describe in a
    mathematical form (model and prior distributions) what we already
    know and what we don't know
\vspace{\baselineskip}
  \item<2-> ``Easy'' part is to use Bayes rule to update the uncertainties
    \begin{itemize}
    \item computational challenges
    \end{itemize}
\vspace{\baselineskip}
  \item<3-> Other parts of the art of probabilistic modeling are, for example,
    \begin{itemize}
    \item model checking/assesment: is data in conflict with our prior knowledge?
    \item model choice: which model should we use?
    \item presentation: presenting the model and the results to the application experts
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Example applications}

  \begin{columns}[T] % align columns
 \begin{column}{.49\textwidth}
{  \footnotesize
  \begin{itemize}
 \item Galaxy clusters for cosmology
 \item Coagulation of blood
 \item Gene regulation
 \item Pharmacokinetics and -dynamics
 \item Decision support
 \item Effects of nutrition for diabetes
 \item Evolutionary anthropology
 \item Clinical trial designs
 \item Daily demand for gas
 \item Brain structure trees
 \item School enrollment
 \item Sports
 \item Product demand
 \end{itemize}
 }
\end{column}
 \begin{column}{.49\textwidth}
{    \footnotesize
  \begin{itemize}
 \item Cocoa bean fermentation
 \item Marine propulsion power
 \item Alcohol consumption trends
 \item Flood probability
 \item Instantaneous heart rate distributions
 \item Drug dosing regimens in pediatrics
 \item Human T stem cell memory cells
 \item Fairness in university admission policies
 \item Destruction of bacteria and bacterial spores under heat
 \end{itemize}
 }
\end{column}
\end{columns}
\end{frame}

%\begin{frame}
%  \frametitle{Bayesian data analysis}  %
%  \framesubtitle{Example analyses}
%  \begin{itemize}
%  \item Treatment/control
%    \begin{itemize}
%    \item randomize patients to treatment or control
%    \item is the treatment effective?
%    \end{itemize}
%    \pause
%  \item Continuous valued treatment
%    \begin{itemize}
%    \item randomize patients with different dosages
%    \item which dosage is sufficient without too many side effects?
%    \end{itemize}
%    \pause
%  \item Different effects for different patients?
%    \begin{itemize}
%    \item Is the treatment effect different for male/female, child/adult, light/heavy, ...
%    \end{itemize}
%  \end{itemize}

%\end{frame}

\begin{frame}{Benefits and problems when going Bayesian}

  \begin{itemize}
  \item Benefits of Bayesian approach
    \begin{itemize}
    \item uncertainty is the foundation
    \item easier interpretation of uncertainty intervals
    \item integrate over uncertainties to focus to interesting parts
    \item straight-forward predictive distributions
    \item use relevant prior information
    \item straight-forward hierarchical models
    \item model checking and evaluation
    \item easier to build and evaluate very complex models
    \end{itemize}
    \pause
  \item Complications of Bayesian approach
    \begin{itemize}
    \item most models does not have nice analytical posteriors
    \item we need to \emph{approximate} our posterior
    \item can be computationally costly/take long time to estimate complex models
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Example: Swedish vote intention estimation}

  \begin{itemize}
  \item Estimation of Swedish vote intention\\
  \url{https://www.bottenada.se/}
  \begin{itemize}
  \item approximately 29 000 parameters \\(every party, every week, since the 90ies)
  \item dynamic house bias
  \item autoregressive industry bias
  \item elections are known
  \item composite data
  \end{itemize}
  \pause
  \item But it takes 12 h to estimate the model
  \end{itemize}
\end{frame}


\section{Bayesian Computation}
\frame{\sectionpage}

\begin{frame}{Computation}
  We want to compute expectations with respect to a posterior
  distribution $p(\theta|y)$
  \begin{align*}
    \E_{\theta|y}\left[ g(\theta) \right] = \int p(\theta|y)g(\theta) d\theta
  \end{align*}

  \begin{itemize}
  \item Analytic
    \begin{itemize}
    \item only for very simple models
    \end{itemize}
  \item Monte Carlo, Markov chain Monte Carlo
    \begin{itemize}
    \item generic, high-quality
    \item computationally expensive
    \end{itemize}
  \item Distributional approximations
    \begin{itemize}
    \item e.g. Laplace, variational inference
    \item less generic, but can be much faster with sufficient accuracy
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}

  \frametitle{Probabilistic programming}

  \begin{center}
   \vspace{-\baselineskip}
    \includegraphics[width=5cm]{figs/wide_ensemble.png}\\
   \vspace{2\baselineskip}
  {\large Enables agile workflow for developing probabilistic models}\\
   \vspace{\baselineskip}
   language \, -- \,
   automated inference \, -- \,
   diagnostics
   \vspace{\baselineskip}
  \vfill
    \includegraphics[width=1.5cm]{figs/stan_logo_wide.png}\\
    \url{mc-stan.org}
  \end{center}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Binomial model for treatment/control comparison}

  \begin{itemize}
  \item Two groups of patients: treatment and control
    \begin{itemize}
    \item Binary outcome (1 treatment, 0 no treatment)
    \item Is the treatment useful?
    \end{itemize}

    \item Model:\\
    Likelihood
    \[
    y_0 \sim Bin(\theta_0, N_0)
    \]
    \[
    y_1 \sim Bin(\theta_1, N_1)
    \]
    \pause
    Prior
    \[
    \theta_0 \sim Beta(1, 1)
    \]
    \[
    \theta_1 \sim Beta(1, 1)
    \]
    \pause
    Goal (odds ratio), e.g. $P(OR>1)$
    \[
    OR = \frac{\frac{\theta_1}{1-\theta_1}}{\frac{\theta_0}{1-\theta_0}}
    \]
  \end{itemize}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Binomial model for treatment/control comparison}
  {\footnotesize
  \begin{lstlisting}[language=Stan]
data {
  int<lower=0> N1;
  int<lower=0> y1;
  int<lower=0> N0;
  int<lower=0> y0;
}
parameters {
  real<lower=0,upper=1> theta1;
  real<lower=0,upper=1> theta0;
}
model {
  theta1 ~ beta(1,1);
  theta0 ~ beta(1,1);
  y1 ~ binomial(N1, theta1);
  y0 ~ binomial(N0, theta0);
}
generated quantities {
  real oddsratio;
  oddsratio = (theta1/(1-theta1))/(theta0/(1-theta0));
}
  \end{lstlisting}
}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Binomial model for treatment/control comparison}

  RStanARM
  {\scriptsize
  \begin{lstlisting}
    fit_bin2 <- stan_glm(y/N ~ grp2, family = binomial(),
                         data = d_bin2, weights = N)
  \end{lstlisting}
}
\end{frame}


\begin{frame}{Modeling nature}

  \begin{itemize}
  \item Drop a ball from different heights and measure time
    \pause
    \begin{itemize}
    \item Newton
    \item air resistance, air pressure, shape and surface structure of the ball
    \item relativity
    \end{itemize}
    \pause
  \item Taking into account the accuracy of the measurements, how
    accurate model is needed?
    \pause
    \begin{itemize}
    \item often simple models are adequate and useful
    \item \emph{All models are wrong, but some of them are useful},
      George P. Box
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Recap: Uncertainty and probabilistic modeling}

  \begin{itemize}
  \item Two types of uncertainty: aleatoric and epistemic
    \vspace{\baselineskip}
  \item Representing uncertainty with probabilities
    \vspace{\baselineskip}
  \item Updating uncertainty using data
    \vspace{\baselineskip}
  %   \pause
  % \item Additional reading material in Noppa (see Lecture 1)
   \end{itemize}
\end{frame}

%\begin{frame}{Questions}
%
%  \begin{itemize}
%  \item<1-> Pick a number between 1--5
%    \begin{itemize}
%    \item<2-> raise as many fingers
%    \item<3-> is the number of fingers raised random (by you or by others)?
%    \end{itemize}
%    \vspace{\baselineskip}
%  \item<4-> If we build a robot with very fast vision which can observe
%    the rotating coin accurately, is the throw random for the robot?
%    \vspace{\baselineskip}
%  \item<5-> Is the quantum uncertainty aleatoric or epistemic?
%    \vspace{\baselineskip}
%  \item<6-> What is your own example with both aleatoric and epistemic
%    uncertainty?
%  \end{itemize}
%\end{frame}

\begin{frame}{Rest of the course}

  \begin{itemize}
  \item Basic models which can be used as building blocks for complex ones
    \vspace{\baselineskip}
  \item Basic computation of posterior and predictive distributions
    \vspace{\baselineskip}
  \item Typical simple scientific data analysis cases
     \begin{itemize}
     \item e.g. comparison of treatments
     \end{itemize}
    \vspace{\baselineskip}
  \item Presentation of the results
  \end{itemize}
\end{frame}

%\begin{frame}{Some important terms}
% This they should know
%  \begin{itemize}
%  \item[-] probability
%  \item[-] probability density function (pdf)
%  \item[-] probability mass function (pmf)
%  \item[-] probability distribution
%  \item[-] discrete probability distribution
%  \item[-] continuous probability distribution
%  \item[-] cumulative distribution function (cdf)
%  \item[-] likelihood
%  \end{itemize}
%
%\end{frame}

%\begin{frame}{Ambiguous notation in statistics}
%
%  \begin{itemize}
%  \item[] In $p(y|\theta)$
%    \pause
%  \begin{itemize}
%  \item[-] $y$ can be variable or (observed) value
%    \begin{itemize}
%    \item[] we could clarify by using $p(Y|\theta)$ or $p(y|\theta)$
%    \end{itemize}
%    \pause
%  \item[-] $\theta$ can be variable or value
%    \begin{itemize}
%    \item[] we could clarify by using $p(y|\Theta)$ or $p(y|\theta)$
%    \end{itemize}
%    \pause
%  \item[-] $p$ can be a discrete or continuous function of $y$ or $\theta$
%    \begin{itemize}
%    \item[] we could clarify by using $P_Y$, $P_\Theta$, $p_Y$ or $p_\Theta$
%    \end{itemize}
%    \pause
%\item[-]
%  $P_Y(Y|\Theta=\theta)$ is a probability mass function, sampling distribution, observation model
%    \pause
%\item[-]
%$P(Y=y|\Theta=\theta)$ is a probability
%    \pause
%\item[-]
%$P_\Theta(Y=y|\Theta)$ is a likelihood function (can be discrete or continuous)
%    \pause
%\item[-] $p_Y(Y|\Theta=\theta)$ is a probability density function, sampling distribution, observation model
%    \pause
%\item[-] $p(Y=y|\Theta=\theta)$ is a density
%    \pause
%\item[-] $p_\Theta(Y=y|\Theta)$ is a likelihood function (can be discrete or continuous)
%    \pause
%  \item[-] $y$ and $\theta$ can also be mix of continuous and discrete
%    \pause
%    \item[-] Due to the sloppines sometimes likelihood is used to refer
%$P_{Y,\theta}(Y|\Theta)$, $p_{Y,\theta}(Y|\Theta)$

%  \end{itemize}
%\end{itemize}
%\end{frame}



\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
