%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
%\input{../common/lststan} % Stan listing
\usepackage{lstbayes}
\usepackage[all,poly,ps,color]{xy}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage{../common/beamerthemeUppsala}
%\usetheme{Uppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath,amsfonts,amssymb}
% \usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

% Input new commands
\input{../common/commands.tex}

\def\dashxy(#1){%
  /xydash{[#1] 0 setdash}def}
\def\grayxy(#1){%
  /xycolor{#1 setgray}def}
\newgraphescape{D}[1]{!{\ar @*{[!\dashxy(2 2)]} "#1"}}
\newgraphescape{P}[1]{!{\ar "#1"}}
\newgraphescape{F}[1]{!{*+=<2em>[F=]{#1}="#1"}}
\newgraphescape{O}[1]{!{*+=<2em>[F]{#1}="#1"}}
\newgraphescape{V}[1]{!{*+=<2em>[o][F]{#1}="#1"}}
\newgraphescape{B}[3]{!{{ "#1"*+#3\frm{} }.{ "#2"*+#3\frm{} } *+[F:!\grayxy(0.75)]\frm{}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Bayesian Statistics and Data Analysis \\ Lecture 7}}
\author[]{M{\aa}ns Magnusson \\ Department of Statistics, Uppsala University \\ Thanks to Aki Vehtari, Aalto University}
\date{}

\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hierarchical models}
\frame{\sectionpage}

\begin{frame}

\frametitle{Hierarchical model}

  \begin{itemize}
  \item Example: Treatment effectiveness
    \begin{itemize}
    \item in hospital $j$ the survival probability is $\theta_j$
    \item observations $y_{ij}$ tell whether patient $i$ survived in
      hospital $j$
        \begin{xy}
          \xymatrix{ \theta_1\ar[d] & \theta_2\ar[d] &
            \cdots & \theta_n\ar[d] \\
            y_{i1} & y_{i2} & & y_{in} }
        \end{xy}
        \pause
      \item sensible to assume that $\theta_j$ are similar
        \begin{xy}
          \xymatrix{
            & \tau \ar[dl] \ar[d] \ar[drr] & & &  \\
            \theta_1\ar[d] & \theta_2\ar[d] &
            \cdots & \theta_n\ar[d] \\
            y_{i1} & y_{i2} & & y_{in} }
        \end{xy}
%        \pause
      \item natural to think that $\theta_j$ have common population distribution
      \item $\theta_j$ is not directly observed and the population distribution is unknown
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]

\frametitle{Hierarchical model: terms}

\begin{overlayarea}{8cm}{4cm}
  \begin{itemize}
  \item[Lvl 1:] observations given parameters $p(y_{ij}|\theta_j)$
  \item<2->[Lvl 2:] parameters given hyperparameters
    $p(\theta_j|\tau)$
  \end{itemize}
  \begin{minipage}[b]{3cm}
  \scriptsize
    \begin{xy}
      \xymatrix{
        p(\tau) & & \tau \ar[dl] \ar[d] \ar[drr] & & & \text{hyperparameter} \\
        p(\theta_j|\tau) & \theta_1\ar[d] & \theta_2\ar[d] & \cdots &
        \theta_n\ar[d] & \text{parameters} \\
        p(y_{ij}|\theta_j) & y_{i1} & y_{i2} &  & y_{in} & \text{observations}
      }
    \end{xy}
  \end{minipage}\\
%  \only<1>{\hide[white]{-0.3}{-0.2}{0.13}{1.2}} %
\end{overlayarea}
\vspace{1cm}
  Joint posterior
  \begin{eqnarray*}
    p(\theta,\tau|y) & \propto & p(y|\theta,\tau) p(\theta,\tau) \\
    & \propto & p(y|\theta) p(\theta|\tau) p(\tau)
  \end{eqnarray*}
\end{frame}

\begin{frame}

\frametitle{Comparisons}
	\begin{itemize}
    \item<+-> "Separate model" (model with separate/independent effects)
        \begin{xy}
          \xymatrix{ \theta_1\ar[d] & \theta_2\ar[d] &
            \cdots & \theta_n\ar[d] \\
            y_{1} & y_{2} & & y_{n} }
        \end{xy}
     \item<+-> "Joint/pooled model" (model with a common effect / pooled model)
        \begin{xy}
          \xymatrix{
            & \theta \ar[dl] \ar[d] \ar[drr] & & &  \\
            y_{1} & y_{2} & \cdots & y_{n}}        \end{xy}
      \item<+-> Hierarchical model\\
        \vspace{-.5\baselineskip}\hspace{0cm}
        \begin{xy}
          \xymatrix{
            & \tau \ar[dl] \ar[d] \ar[drr] & & &  \\
            \theta_1\ar[d] & \theta_2\ar[d] &
            \cdots & \theta_n\ar[d] \\
            y_{1} & y_{2} & & y_{n} }
        \end{xy}
  \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Predictive distributions for hiearchical models}

  \begin{itemize}
    \item Two types of predictive distributions
    \begin{enumerate}
      \item A new observation in {\color{uured} an existing group}
      \item A new observation in {\color{uured} a new group}
    \end{enumerate}
        \begin{xy}
          \xymatrix{
            & \tau \ar[dl] \ar[d] \ar[drr] & & &  \\
            \theta_1\ar[d] & \theta_2\ar[d] &
            \cdots & \theta_n\ar[d] \\
            y_{1} & y_{2} & & y_{n} }
        \end{xy}
  \end{itemize}
\end{frame}

\subsection{Rats example}

\begin{frame}

\frametitle{Hierarchical binomial model: rats}

  \begin{itemize}
  \item Medicine testing
  \item Type F344 female rats in control group given placebo
    \begin{itemize}
    \item count how many get endometrial stromal polyps
    \item familiar binomial model example
    \end{itemize}
  \item<2-> Experiment has been repeated 71 times
    {\tiny
      \begin{tabular}{r r r r r r r r r r}
        0/20 & 0/20 & 0/20 & 0/20 & 0/20 & 0/20 & 0/20 & 0/19 & 0/19 & 0/19 \\
        0/19 & 0/18 & 0/18 & 0/17 & 1/20 & 1/20 & 1/20 & 1/20 & 1/19 & 1/19 \\
        1/18 & 1/18 & 2/25 & 2/24 & 2/23 & 2/20 & 2/20 & 2/20 & 2/20 & 2/20 \\
        2/20 & 1/10 & 5/49 & 2/19 & 5/46 & 3/27 & 2/17 & 7/49 & 7/47 & 3/20 \\
        3/20 & 2/13 & 9/48 & 10/50 & 4/20 & 4/20 & 4/20 & 4/20 & 4/20 & 4/20 \\
        4/20 & 10/48 & 4/19 & 4/19 & 4/19 & 5/22 & 11/46 & 12/49 & 5/20 & 5/20 \\
        6/23 & 5/19 & 6/22 & 6/20 & 6/20 & 6/20 & 16/52 & 15/46 & 15/47 & 9/24 \\
        4/14 &      &      &      &      &      &       &       &       &
      \end{tabular}}
  \end{itemize}

\end{frame}

\begin{frame}

\frametitle{Hierarchical binomial model: rats}

  \only<1>{\includegraphics[width=9cm]{figs/rats_pooled.pdf}}
  \only<2>{\includegraphics[width=9cm]{figs/rats_separate.pdf}}


\end{frame}

\begin{frame}[fragile]

\frametitle{Hierarchical binomial model: rats}

  \begin{itemize}
  \item Hierarchical binomial model for rats\\
    prior parameters {\color{red} $\alpha$} and {\color{red} $\beta$} are unknown
\end{itemize}
    \begin{minipage}[t]{4cm}
      \xygraph{
        []              !O{y_j}
        ([u]   !V{{\color{uulgr}\theta_j}}  !P{y_j}
        ([u][l(0.33)] !V{\color{red} \alpha} !P{{\color{uulgr}\theta_j}},
        [u][r(0.33)] !V{\color{red} \beta}  !P{{\color{uulgr}\theta_j}}),
        [u][r(0.75)]  !F{n_j}    !P{y_j},
        [r(1.0)]*{j},
        [l(3.0)]*{y_j|n_j,{\color{uulgr}\theta_j} \sim \Bin(y_j|n_j,{\color{uulgr}\theta_j})},
        [u][l(2.93)]*{{\color{uulgr}\theta_j}|{\color{red}\alpha,\beta} \sim
          \Beta({\color{uulgr}\theta_j}|{\color{red}\alpha,\beta})},
        [l(6.0)]*{~}
        )
        !B{{\color{uulgr}\theta_j}}{j}{++}
      }
    \end{minipage}
    %\pause
  \begin{itemize}
    \item Joint posterior
      $p({\color{uulgr}\theta_1,\ldots,\theta_J},{\color{red}\alpha},{\color{red}\beta}|y)$
      \begin{itemize}
    \item multiple parameters
      \pause
      \item factorize
        $\prod_{j=1}^J p({\color{uulgr}\theta_j}|{\color{red}\alpha,\beta},y)p({\color{red}\alpha,\beta}|y)$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}

\frametitle{Hierarchical binomial model: rats}

  \begin{itemize}
  \item Population prior $\Beta({\color{uulgr}\theta_j}|{\color{red}\alpha,\beta})$
  \item Hyperprior $p({\color{red}\alpha,\beta})$?
    \begin{itemize}
    \item ${\color{red}\alpha,\beta}$ both affect the location and scale
    \item BDA3 (p. 110) has (vague)
      $p({\color{red}\alpha,\beta})\propto({\color{red}\alpha}+{\color{red}\beta})^{-5/2}$
    \end{itemize}
    \pause
    \item {\color{uured} What type of predicitive distributions can we have?}
    \end{itemize}
\end{frame}

\begin{frame}

\frametitle{Hierarchical binomial model: rats}

  \only<1>{\includegraphics[width=8cm]{figs/rats_posterior.pdf}}
  \only<2>{\includegraphics[width=8cm]{figs/rats_hierdraws.pdf}}
  \only<3>{\includegraphics[width=8cm]{figs/rats_hierprior.pdf}}
  \only<4-5>{\vspace{-0.3\baselineskip}\includegraphics[width=6.7cm]{figs/rats_separate_less.pdf}\\}
  \only<5-6>{\vspace{-0.3\baselineskip}
    \includegraphics[width=6.7cm]{figs/rats_hier_less.pdf}\\}
  \only<6>{\vspace{-0.3\baselineskip}
    \includegraphics[width=6.7cm]{figs/rats_hierprior.pdf}\\}


\end{frame}

\subsection{Factory example}

\begin{frame}

\frametitle{Hierarchical normal model: factory}

  \begin{itemize}
  \item Factory has 6 machines which quality is evaluated
  \item Assume hierarchical model
    \begin{itemize}
    \item each machine has its own (average) quality $\theta_j$ and
      {\color{uured} common variance} $\sigma^2$
    \end{itemize}
\begin{minipage}[b]{4cm}
\begin{xy}
\xygraph{
  []              !O{y_{ij}}
 ([u][l(0.75)]   !V{\theta_j}      !P{y_{ij}}
 ([u][l(0.33)] !V{\mu_P}      !P{\theta_j},
   [u][r(0.33)] !V{\sigma_P^2} !P{\theta_j}),
  [u][r(1.75)]   !V{\sigma^2}   !P{y_{ij}},
  [r(0.45)]*{i},
  [r(0.9)]*{j},
[l(3)]*{y_{ij}|\theta_j \sim \N(\theta_j,\sigma^2)},
[u][l(3)]*{\theta_j|\mu_P,\sigma_P^2 \sim \N(\mu_P,\sigma_P^2)},
)
!B{y_{ij}}{i}{}
!B{\theta_j}{j}{++}
 }
\end{xy}
    \end{minipage}
  \item Can be used to predict the future quality produced by each machine and quality produced by a new similar machine
  \end{itemize}
\end{frame}

\begin{frame}

\frametitle{Hierarchical normal model: factory}

  \begin{itemize}
  \item Factory has 6 machines which quality is evaluated
  \item Assume hierarchical model
    \begin{itemize}
    \item each machine has its own (average) quality $\theta_j$ and
      {\color{uured} own variance} $\sigma_j^2$
    \end{itemize}
\hspace{-1cm}~\begin{minipage}[b]{4cm}
      \begin{xy}
        \xygraph{
          []              !O{y_{ij}}
          ([u][l(0.75)]   !V{\theta_j}      !P{y_{ij}}
          ([u][l(0.33)] !V{\mu_P}      !P{\theta_j},
          [u][r(0.33)] !V{\sigma_P^2} !P{\theta_j}),
          [u][r(0.75)]   !V{\sigma_j^2}   !P{y_{ij}}
          ([u][l(0.33)] !V{\sigma_0^2}      !P{\sigma_j^2},
          [u][r(0.33)] !V{\nu_0} !P{\sigma_j^2}),
          [r(0.45)]*{i},
          [r(0.9)]*{j},
          [l(3.0)]*{y_{ij}|\theta_j \sim \N(\theta_j,\sigma_j^2)},
          [u][l(3.0)]*{\theta_j|\mu_P,\sigma_P^2 \sim \N(\mu_P,\sigma_P^2)},
          [u][r(3.25)]*{\sigma_j^2|\sigma_0^2,\nu_0 \sim \Invchi2(\sigma_0^2,\nu_0)}
          )
          !B{y_{ij}}{i}{}
          !B{\theta_j}{j}{++}
        }
      \end{xy}
    \end{minipage}
    \pause
  \item {\color{uured} What type of predicitive distributions can we have?}
    \pause
  \item Can be used to predict the future quality produced by each machine and quality produced by a new similar machine
  \end{itemize}
\end{frame}

\subsection{8 schools example}

\begin{frame}

\frametitle{Hierarchical normal model: 8 schools}

  \begin{itemize}
  \item Example: SAT coaching effectiveness
    \begin{itemize}
    \item in USA commonly used Scholastic Aptitude Test (SAT) is
      designed so that short term practice should not improve the
      results significantly
    \item schools have anyway coaching courses
    \item test the effectiveness of the coaching courses
    \end{itemize}
    \pause
  \item SAT
    \begin{itemize}
    \item standardized multiple choice test
    \item mean about 500 and standard deviation about 100
    \item most scores between 200 and 800
    \item different topics, e.g., V=Verbal, M=Mathematics
    \item pre-test PSAT
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}

\frametitle{Hierarchical normal model: 8 schools}

  \begin{itemize}
  \item Effectiveness of the SAT coaching
    \begin{itemize}
    \item students had made pre-tests PSAT-M and
      PSAT-V
    \item part of students were coached
    \item linear regression was used to estimate the coaching effect
      $y_j$ for the school $j$ (could be denoted with $\bar{y}_{.j}$,
      too) and variances $\sigma_j^2$
    \item $y_j$ approximately normally distributed, with variances
      assumed to be known based on about 30 students per school
    \item data is group means and variances (not personal results)
    \end{itemize}
    \pause
  \item Data:
    {\small
    \begin{tabular}[t]{r | r r r r r r r r}
      School & A & B & C & D & E & F & G & H \\
      $y_j$ & 28 & 8 & -3 & 7 & -1 & 1 & 18 & 12 \\
      $\sigma_j$ & 15 & 10 & 16 & 11 & 9 & 22 & 20 & 28
    \end{tabular}}
  \end{itemize}
\end{frame}

\begin{frame}

\frametitle{Hierarchical normal model for group means}

  \begin{itemize}
  \item $J$ experiments, unknown $\theta_j$ and known $\sigma^2$
    \begin{equation*}
      y_{ij}|\theta_j \sim \N(\theta_j,\sigma^2), \quad
      i=1,\ldots,n_j; \quad j=1,\ldots,J
    \end{equation*}
    \vspace{-6mm}
  \item Group $j$ sample mean and sample variance
    \begin{eqnarray*}
      \bar{y}_{.j} & = & \frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}\\
      \sigma_j^2 & = & \frac{\sigma^2}{n_j}
    \end{eqnarray*}
    \vspace{-6mm}
    \pause
  \item Use model
    \begin{eqnarray*}
      \bar{y}_{.j}|\theta_j \sim \N(\theta_j,\sigma_j^2)
    \end{eqnarray*}
     this model can be generalized so that, $\sigma_j^2$ can be
    different from each other for other reasons than $n_j$
  \end{itemize}
\end{frame}

\begin{frame}

\frametitle{Hierarchical normal model for group means}

    \begin{minipage}[b]{4cm}
      \begin{xy}
        \xygraph{
          []              !O{\bar{y}_{.j}}
          ([u][l(0.5)]   !V{\theta_j}      !P{\bar{y}_{.j}}
          ([u][l(0.33)] !V{\mu}      !P{\theta_j},
          [u][r(0.33)] !V{\tau} !P{\theta_j}),
          [u][r(0.5)]   !O{\sigma_j^2}   !P{\bar{y}_{.j}},
  [r(1.0)]*{j},
  [l(3.0)]*{\bar{y}_{.j}|\theta_j \sim \N(\theta_j,\sigma^2_j)},
  [u][l(3.0)]*{\theta_j|\mu,\tau \sim \N(\mu,\tau)}
  )
  !B{\theta_j}{j}{++}
}
\end{xy}
\end{minipage}
\end{frame}

\begin{frame}

\frametitle{Hierarchical normal model: 8 schools}

  \only<1-3>{\includegraphics[width=8.3cm]{figs/8schools_separate.pdf}\\}
  \only<2-3>{\includegraphics[width=8.3cm]{figs/8schools_pooled.pdf}\\}
  \only<3>{\includegraphics[width=8.3cm]{figs/8schools_hier.pdf}}
  \only<4-6>{\includegraphics[width=5.5cm]{figs/8schools_tau.pdf}\\}
  \only<5-6>{\includegraphics[width=5.5cm]{figs/8schools_condmu.pdf}\\}
  \only<6>{\includegraphics[width=5.5cm]{figs/8schools_condsd.pdf}}

\end{frame}


\section{Exchangeability}
\frame{\sectionpage}

\begin{frame}

\frametitle{Exchangeability}

  \begin{itemize}
  \item Justifies why we can use
    \begin{itemize}
    \item a joint model for data
    \item a joint prior for a set of parameters
    \end{itemize}
  \item Less strict than independence (IID)
  \item IID $\rightarrow$ exchangeability
  \end{itemize}
\end{frame}

\begin{frame}

\frametitle{Exchangeability}

  \begin{itemize}
  \item \textit{Exchangeability} \\ Parameters
    $\theta_1,\ldots,\theta_J$ (or observations $y_1,\ldots,y_J$) are
    exchangeable if the joint distribution $p$ is invariant to the
    permutation of indices $(1,\ldots,J)$\\
  e.g.
    \begin{equation*}
      p(\theta_1,\theta_2,\theta_3) = p(\theta_2,\theta_3,\theta_1)
    \end{equation*}
\pause
  \uured{Can we come up with a situation where this doesn't hold?}
\pause
  \item Exchangeability implies symmetry: \\If there is no information
    which can be used \textit{a priori} to separate $\theta_j$ form
    each other, we can assume exchangeability. ("Ignorance implies exchangeability")

  \end{itemize}
\end{frame}


\begin{frame}

\frametitle{Exchangeability}

  \begin{itemize}
  \item Exchangeability does not mean that the results of the
    experiments could not be different
    \begin{itemize}
    \item e.g. if we know that the experiments have been in two
      different laboratories, and we know that the other laboratory
      has better conditions for the rats, but we do not know which
      experiments have been made in which laboratory
    \item a priori experiments are exchangeable
    \item model could have unknown parameter for the laboratory with a
      conditional prior for rats assumed to come form the same place (clustering model)
    \end{itemize}
  \end{itemize}
\end{frame}

% \note{Elinolojen parantaminen vaikutti
\begin{frame}

\frametitle{Exchangeability and additional information}

  \begin{itemize}
  \item Example: bioassay
    \begin{itemize}
      \item<+-> $y_i$ number of dead animals are not exchangeable alone
      \item<+-> $x_i$ dose is additional information
      \item<+-> $(x_i,y_i$) exchangeable and logistic regression was used
    \begin{align*}
      p(\alpha,\beta|y,n,x)\propto \prod_{i=1}^n p(y_i|\alpha,\beta,n_i,x_i)p(\alpha,\beta)
    \end{align*}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}

\frametitle{Hierarchical exchangeability}

  \begin{itemize}
  \item Example: hierarchical rats example
    \begin{itemize}
    \item<+-> all rats not exchangeable
    \item<+-> in a single laboratory rats exchangeable
    \item<+-> laboratories exchangeable

    \pause

    $\rightarrow$ hierarchical model can be used
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}

\frametitle{Partial or conditional exchangeability}

\begin{itemize}
  \item Conditional exchangeability
    \begin{itemize}
    \item if $y_i$ is connected to an additional information $x_i$, so
      that $y_i$ are not exchangeable, but $(y_i,x_i)$ exchangeable
      use joint model or conditional model $(y_i|x_i)$.
    \end{itemize}
  \item<2-> Partial exchangeability
    \begin{itemize}
    \item if the observations can be grouped (a priori), then we can use a
      hierarchical model
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}

\frametitle{Exchangeability}

  \begin{itemize}
  \item The simplest form of the exchangeability (but not the only
    one) for the parameters $\theta$ is \uured{conditional independence}
    \begin{equation*}
      p(x_1,\ldots,x_J|\theta)=\prod_{j=1}^J p(x_j|\theta)
    \end{equation*}
  \item<2-> Let $(x_n)_{n=1}^{\infty}$ to be an infinite sequence of
    exchangeable random variables. De Finetti's theorem then says that
    there is some random variable $\theta$ so that $x_j$ are
    conditionally independent given $\theta$, and joint density for
    $x_1,\ldots,x_J$ can be written in the \textit{iid mixture} form
    \begin{equation*}
      p(x_1,\ldots,x_J)=\int \left[\prod_{j=1}^J p(x_j|\theta)\right]p(\theta)d\theta
    \end{equation*}
   \end{itemize}
\end{frame}

%\begin{frame}

%\frametitle{Exchangeability - Counter example}

%  \begin{itemize}
%  \item A six sided die with probabilities (a finite sequence!)
%    $\theta_1,\ldots,\theta_6$
%    \begin{itemize}
%    \item without additional knowledge $\theta_1,\ldots,\theta_6$
%      exchangeable
%    \item due to the constraint $\sum_{j=1}^6\theta_j$, parameters
%      are not independent and thus joint distribution can not be
%      presented as iid mixture
%    \end{itemize}
%  \end{itemize}
%\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Computational aspects}
\frame{\sectionpage}

\begin{frame}
\frametitle{The funnel posterior }

  \begin{itemize}
  \item Hiearchical models
  \begin{itemize}
  \item Group-level or global parameters, e.g.
  \[
  \tau \sim p(\tau)
  \]
  \item Local or individual-level parameters
  \[
  \theta_i \sim \mathcal{N}(0, \tau)
  \]
  \end{itemize}
  \item Creates a "funnel-like" posterior geometry:
  %  This causes the joint posterior distribution to have the shape of a funnel: wide at the top (corresponding to high values of \(\tau\)) and narrow at the bottom (corresponding to small values of \(\tau\)).
  \item Comes from the variance in the different layers:
  \begin{itemize}
  \item When $\tau$ is small, the $\theta_i$'s are concentrated around 0
  \item When $\tau$ is large, the $\theta_i$'s are widely dispersed
  \end{itemize}
  \end{itemize}
\end{frame}


\end{document}



