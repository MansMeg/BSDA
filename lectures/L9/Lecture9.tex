%\documentclass[10pt,handout]{beamer}
\documentclass[10pt]{beamer}
\usepackage{babel} % Anpassa efter svenska. Ger svensk logga.
\usepackage[utf8]{inputenc} % Anpassa efter linux
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
%\input{../common/lststan} % Stan listing
\usepackage{lstbayes}
\usepackage[all,poly,ps,color]{xy}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage{../common/beamerthemeUppsala}
%\usetheme{Uppsala}
%\usecolortheme{UU} % Anpassa efter UU:s frger och logga
%\hypersetup{pdfpagemode=FullScreen} % Adobe Reader ska ppna fullskrm
\setbeamertemplate{itemize items}[circle]

% \usepackage{beamerthemesplit}
\usepackage{amsmath,amsfonts,amssymb}
% \usepackage{amssymb}
% \usepackage{graphics}
% \usepackage{graphicx}
% \usepackage{epsfig}
% \usepackage[latin1]{inputenc}
 \usepackage{color}
% \usepackage{fancybox}
% \usepackage{psfrag}
% \usepackage[english]{babel}
 \setbeamertemplate{footline}{\hfill\insertframenumber/\inserttotalframenumber}

% Input new commands
\input{../common/commands.tex}

\def\dashxy(#1){%
  /xydash{[#1] 0 setdash}def}
\def\grayxy(#1){%
  /xycolor{#1 setgray}def}
\newgraphescape{D}[1]{!{\ar @*{[!\dashxy(2 2)]} "#1"}}
\newgraphescape{P}[1]{!{\ar "#1"}}
\newgraphescape{F}[1]{!{*+=<2em>[F=]{#1}="#1"}}
\newgraphescape{O}[1]{!{*+=<2em>[F]{#1}="#1"}}
\newgraphescape{V}[1]{!{*+=<2em>[o][F]{#1}="#1"}}
\newgraphescape{B}[3]{!{{ "#1"*+#3\frm{} }.{ "#2"*+#3\frm{} } *+[F:!\grayxy(0.75)]\frm{}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setlength{\parskip}{3mm}
\title[]{{\color{black}Bayesian Statistics and Data Analysis \\ Lecture 9}}
\author[]{M{\aa}ns Magnusson \\ Department of Statistics, Uppsala University \\ Thanks to Aki Vehtari, Aalto University}
\date{}

\begin{document}

\frame{\titlepage
% \thispagestyle{empty}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Bayesian decision theory}
\frame{\sectionpage}


\begin{frame}

\frametitle{Bayesian decision theory}

  \begin{itemize}
  \item<+-> Potential decisions (or actions) $d \in D$ ($a$)
  \pause
  \item<+-> Potential consequences (or outcomes) $x$
    \begin{itemize}
      \item $x$ may be categorical, ordinal, real, scalar, vector, etc.
    \end{itemize}
    \pause
  \item<+-> Probability distribution of outcome given decision $p(x|d)$
    \begin{itemize}
    \item in decision making the decisions are controlled and thus $p(d)$ does not exist/are fixed
    \end{itemize}
    \pause
  \item<+->  Utility function $U(x)$ maps consequences to real value
    \begin{itemize}
      \item e.g. euro or expected lifetime
      \item instead of utility sometimes cost or loss is defined
    \end{itemize}
  \pause
  \item<+-> Expected utility for a decision $d$
  \[
  E[U(x)|d]=\int U(x) p(x|d) dx
  \]
  \pause
  \item<+-> \uured{Optimal decision}: $d^*$, which maximizes the expected utility
    \begin{equation*}
      d^*=\arg\max_d E[U(x)|d]
    \end{equation*}
  \end{itemize}

\end{frame}

\begin{frame}

\frametitle{Integrating inference and decisions}

  \begin{itemize}
    \item<+-> To make an optimal decision we need $p(x|d)$
    \pause
    \item<+-> In many situations we can approximate $p(x|d) \approx p(x)$
    \pause
    \item<+-> The benefit of Bayesian inference: We can use $p(x|d, y)$ i.e. integrating data in decision making
  \end{itemize}

\end{frame}

\begin{frame}

\frametitle{Challenges in decision making}

  \begin{itemize}
  \item Actual utility functions are rarely linear
  \item<2-> What is the cost of human life?
  \item<3-> Multipel parties having different utilities
  \end{itemize}

\end{frame}

\section{Examples}
\frame{\sectionpage}

\begin{frame}

\frametitle{Example of decision making: 2 choices}

\begin{itemize}
\item<+-> Helen is going to pick mushrooms in a forest, while she notices a paw print which could made by a dog or a wolf
\item<+-> Helen measures that the length of the paw print is 14 cm and
  goes home to Google how big paws dogs and wolves have, and tries
  then to infer which animal has made the paw print
  \includegraphics[width=8cm]{figs/hatutus_likelihoods}
  observed length has been marked with a horizontal line
\item<+-> Likelihood of wolf is 0.92 (alternative being dog)
\end{itemize}

\end{frame}



\begin{frame}

\frametitle{Example of decision making}

  \begin{itemize}
  \item<+-> Helen assumes also that in her living area there are about one hundred times more free running dogs than wolves, that is {\em a
      priori} probability for wolf, before observation is 1\%.
  \item<+-> Likelihood and posterior
    \begin{center}\leavevmode
      \begin{tabular}{| l | c c |}
        \hline
        Animal &  Likelihood & Posterior probability \\
        \hline
        Wolf     &  0.92            & 0.10      \\
        Dog    &  0.08        & 0.90    \\
        \hline
      \end{tabular}
    \end{center}
  \item<+-> Posterior probability of wolf is 10\%
  \end{itemize}

\end{frame}

\begin{frame}

\frametitle{Example of decision making}

  \begin{itemize}
  \item<+-> Helen has to make decision whether to go pick mushrooms
  \item<+-> If she doesn't go to pick mushrooms utility is zero
  \item<+-> Helen assigns positive utility 1 for getting fresh mushrooms
  \item<+-> Helen assigns negative utility -1000 for a event that she goes to the forest and wolf attacks (for some reason Helen assumes that wolf will always attack)\\
    \vspace{\baselineskip}
    \uncover<+->{
    \begin{minipage}[t]{58mm}
      \small
      \begin{tabular}{| l | c c |}
        \hline
        & \multicolumn{2}{ c |}{Animal} \\
        Decision $d$           & Wolf & Dog \\
        \hline
        Stay home             & 0    & 0              \\
        Go to the forest      & -1000 & 1      \\
        \hline
      \end{tabular}\\
      {Utility matrix $U(x)$}
    \end{minipage}
    }
    ~\\
    \vspace{\baselineskip}
    \uncover<+->{
    \begin{minipage}[t]{58mm}
      \small
      \begin{tabular}{| l | c | }
        \hline
        & Conditional utility \\
        Action $d$        &  $E[U(x)|d]$ \\
        \hline
        Stay home         &  0       \\
        Go to the forest  &  -100+0.9     \\
        \hline
      \end{tabular}\\
      {Utilities for different actions}
    \end{minipage}
}
  \end{itemize}

\end{frame}

\begin{frame}

\frametitle{Example of decision making}

  \begin{itemize}
  \item<+-> Maximum likelihood decision would be to assume that there is a wolf
  \pause
  \item<+-> Maximum posterior decision would be to assume that there is a dog
  \pause
  \item<+-> Maximum utility decision is to stay home, even if it is more likely that the animal is dog
    \pause
  \item<+-> Example illustrates that the uncertainties (probabilities)
    related to all consequences need to be carried on until final
    decision making
  \end{itemize}

\end{frame}



\begin{frame}

\frametitle{Multi-stage decision making (Section 9.3)}

  \begin{itemize}
  \item<+-> 95 year old has a tumor that is malignant with 90\% probability
  \item<+-> Based on statistics
    \begin{itemize}
    \item<.-> expected lifetime is 34.8 months if no cancer
    \item<+-> expected lifetime is 16.7 months if cancer and radiation therapy is used
    \item<+-> expected lifetime is 20.3 months if cancer and surgery, but the probability of dying in surgery is 35\% (for 95 year old)
    \item<+-> expected lifetime is 5.6 months is cancer and no treatment
    \end{itemize}
  \item<+-> Which treatment to choose?
    \begin{itemize}
    \item<.-> quality adjusted life time
    \item<.-> 1 month is subtracted for the time spent in treatments
    \end{itemize}
   \item<+-> Quality adjusted life time
    \begin{itemize}
    \item<.-> Radiothreapy: 0.9*16.7 + 0.1*34.8 - 1 = 17.5mo
    \item<+-> Surgery: 0.35*0 + 0.65*(0.9*20.3 + 0.1*34.8 - 1) = 13.5mo
    \item<+-> No treatment: 0.9*5.6 + 0.1*34.8 = 8.5mo
    \end{itemize}
  \item<+-> See the book for continuation of the example with
    additional test for cancer
\end{itemize}

\end{frame}

\begin{frame}

\frametitle{Design of experiment}

  \begin{itemize}
  \item Which experiment would give most additional information
    \begin{itemize}
    \item decide values $x_{n+1}$ for the next experiment
    \item which values of $x_{n+1}$ would reduce the posterior uncertainty most
    \end{itemize}
  \item Example
    \begin{itemize}
    \item Imagine that in bioassay the posterior uncertainty of LD50 is too large
    \item which dose should be used in the next experiment to reduce
      the variance of LD50 as much as possible ?
      \begin{itemize}
        \item this way less experiments need to be made (and less animals need to be killed)
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}

\frametitle{Model selection as decision problem}

\begin{itemize}
\item Evaluate how model $M$ \uured{generalizes to unseen data} $\tilde{y}$\\(the \emph{expected log predictive density}):
\[
\text{elpd}_M= \int \log p_M({\color{uured}\tilde{y}}|y) p_\text{true}({\color{uured}\tilde{y}}) d{\color{uured}\tilde{y}}\,,
\]
where ${\color{uured}\tilde{y}}$ is an unseen observation generated from the true data generating process $p_\text{true}({\color{uured}\tilde{y}})$, and $y$ are observed data.
\item $\log p_M({\color{uured}\tilde{y}}|y)$ is the log score (the utility of the model)
\pause
\item Formally, our decision is to choose the model $M^\star$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Automated decision-making and Discrimination}

Work of Holli Sargeant:

\begin{itemize}
\item Automated decision-making in an ML-context
\[
d^\star = \sum_d u(y,d) \hat{p}(y|d,x)\,,
\]
where $y$ is indicates if a person defaults, $d$ is the decision on weather granting a loan or not, $\hat{p}$ is a prediction model, and $u(y,d)$ is the utility of a bank.
\pause
\item Conditional statistical parity
\[
\mathbb{E}_x\left[\hat{p}(y|x) \mid x_l, x_p\right] = \mathbb{E}_x\left[\hat{p}(y|x)\mid x_l\right]\,,
\label{eq_cond_stat_parity}
\]
where $x_l$ is legitimate factors and $x_p$ are protected attributes.
\end{itemize}

\end{frame}


\section{Course Evaluation}
\frame{\sectionpage}

\begin{frame}

\frametitle{Course Evaluation}

\begin{itemize}
\item What was good? What was fun?
\pause
\item What can be improved? What was annoying?
\pause
\item Did you get what you expected?
\pause
\item How can I get you to speak more during class?
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
